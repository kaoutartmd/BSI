






















































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































Training standard_a3c:  26%|███████████████████                                                       | 2580/10000 [43:36:46<125:25:47, 60.86s/it, avg_reward=777.94, influence_w=0.000]
Traceback (most recent call last):
  File "train.py", line 191, in <module>
    if args.env in ['cleanup', 'both']:
  File "train.py", line 170, in main
    help='Path to existing results to analyze instead of training')
  File "train.py", line 125, in run_experiments
  File "/home/kaou-internship/basic_influence_attention/experiments/runner.py", line 275, in run_all_experiments
    config = self.configs['standard_a3c'].copy()
  File "/home/kaou-internship/basic_influence_attention/experiments/runner.py", line 150, in run_single_experiment
  File "/home/kaou-internship/basic_influence_attention/models/attention_influence_a3c.py", line 272, in train_episode
    # *** ATTENTION-BASED INFLUENCE COMPUTATION ***
  File "/home/kaou-internship/basic_influence_attention/models/attention_influence_a3c.py", line 191, in compute_influence_with_attention
    with torch.no_grad():
  File "/home/kaou-internship/anaconda3/envs/binf/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/kaou-internship/anaconda3/envs/binf/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/kaou-internship/basic_influence_attention/models/attention_influence_a3c.py", line 84, in forward
    # Get attention-weighted neighbor representation
  File "/home/kaou-internship/anaconda3/envs/binf/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/kaou-internship/anaconda3/envs/binf/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/kaou-internship/basic_influence_attention/models/attention_influence_a3c.py", line 42, in forward
    embeddings = torch.stack(embeddings)  # [num_neighbors, hidden_dim]
KeyboardInterrupt